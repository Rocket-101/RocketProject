{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from pandas_datareader import data as web\n",
    "import os\n",
    "import string\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Title\n",
    "def GetStockPrice(stock, startDate, endDate, method):\n",
    "    if method == 0:\n",
    "        Stockdf = web.DataReader(stock, 'yahoo', startDate, endDate)\n",
    "        SPX = web.DataReader('^GSPC', 'yahoo', startDate, endDate)\n",
    "        AdjClosedf = pd.DataFrame(Stockdf['Adj Close'].pct_change() - SPX['Adj Close'].pct_change())\n",
    "        AdjClosedf.loc[:,'Label'] = (AdjClosedf['Adj Close']>0.003).astype(int) - (AdjClosedf['Adj Close']< -0.003).astype(int)\n",
    "\n",
    "    else:\n",
    "        page = urllib.request.urlopen('https://finance.yahoo.com/quote/'+stock+'/history?period1=1474095600&period2=1504249200&interval=1d&filter=history&frequency=1d').read()\n",
    "        soup = BeautifulSoup(page, 'html5lib')\n",
    "        snap = soup.find(\"table\", class_=\"W(100%) M(0)\")\n",
    "        snap_body = snap.find('tbody')\n",
    "        rows = snap_body.find_all('tr')\n",
    "\n",
    "        #print \"Number of data: %d\\n\" % (len(rows))\n",
    "        Datelist = []\n",
    "        AdjCloselist = []\n",
    "\n",
    "        for element in rows:\n",
    "            s = element.find_all(\"span\")\n",
    "            if len(s) == 7:\n",
    "                Datelist.append(datetime.strptime(s[0].get_text(), '%b %d, %Y'))\n",
    "                AdjCloselist.append(locale.atof(s[5].get_text().replace(',','')))\n",
    "\n",
    "        AdjClosedf = pd.DataFrame(AdjCloselist,columns=[stock],index = Datelist)\n",
    "        #print(AdjClosedf.columns)\n",
    "        AdjClosedf = AdjClosedf.sort_index()\n",
    "        AdjClosedf.rename(columns={stock: 'Adj Close'}, inplace=True)\n",
    "        AdjClosedf.loc[:,'Label'] = (AdjClosedf['Adj Close'].pct_change()>0).astype(int)\n",
    "        \n",
    "    return AdjClosedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-10-01'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "d = datetime.date(2018,10,1)\n",
    "dt = datetime.date.strftime(d, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-10-01',\n",
       " '2018-09-30',\n",
       " '2018-09-29',\n",
       " '2018-09-28',\n",
       " '2018-09-27',\n",
       " '2018-09-26',\n",
       " '2018-09-25',\n",
       " '2018-09-24',\n",
       " '2018-09-23',\n",
       " '2018-09-22']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = datetime.date(2018,10,1)\n",
    "date_list = [base - datetime.timedelta(days=x) for x in range(0, 10)]\n",
    "date_list\n",
    "a = []\n",
    "for i in date_list:\n",
    "    dt = datetime.date.strftime(i, '%Y-%m-%d')\n",
    "    a.append(dt)\n",
    "    \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping from Wall Street Journal\n",
    "def news(startdate, daterange):\n",
    "    base = datetime.date(startdate)\n",
    "    date_list = [base - datetime.timedelta(days=x) for x in range(daterange)]\n",
    "    date_loop_list = []\n",
    "    for i in date_list:\n",
    "        dt = datetime.date.strftime(i, '%Y-%m-%d')\n",
    "        date_loop_list.append(dt)\n",
    "    \n",
    "    for date in date_loop_list:\n",
    "        url = 'http://www.wsj.com/public/page/archive-'+ date + '.html'\n",
    "        page = requests.get(url)\n",
    "        if not page.status_code == 200:    #  Status Code\n",
    "            return None\n",
    "        try:\n",
    "            result = BeautifulSoup(page.content,'lxml')\n",
    "            UL = result.find_all('ul',class_ = 'newsItem')\n",
    "            body1 = UL[0].find_all('p')\n",
    "            body2 = body1[0].get_text()\n",
    "            body3 = c2.strip().split('\\n ')\n",
    "            Headlines = body3[0]\n",
    "            Abstracts = body3[0][11:]\n",
    "            \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://www.wsj.com/public/page/archive-2017-1-01.html'\n",
    "page = requests.get(url)\n",
    "bs_page = BeautifulSoup(page.content,'lxml')\n",
    "all_links = bs_page.find_all('ul',class_=\"newsItem\")\n",
    "content = all_links[0].find_all('p')\n",
    "c1 = content[0]\n",
    "c2 = c1.get_text()\n",
    "c3 = c2.strip().split('\\n ')\n",
    "c3[1][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
